This is where I began my journey as a data miner /not a scientist :(

This is a Kaggle competition(the very first Kaggle competition) I have accomplished when I was a sophomore. It feels so great to take a look back at it again! I learnt so much from dealing with really messy data in industries these years. A clean and well-prepared dataset is a bless!

There are so many methods I keep reusing in almost every data-driven project I have done in the past few years. 

* EDA methods: missing_values_table(df) and all kinds of plots in EDAonTrainandTest.ipynb
* Early stopping concept in Light Gradient boosting 

Quote from a algorithm engineer from Tencent Music who interviewed me (I did not get the job becuz I am in New York and Tencent is in Shenzhen(time zone conflict), plus I did not do so well in the coding test), "We don't spend too much time on model tuning or searching for hyperparameters. There are more important things to do such as finding the key features! (or pre-train model if we don't have enough data)" 

Yeah, domain knowledge counts a lot. When I was a quant research intern in Guolian Security, I tried hard to learnt as much as possible about the stock market to include more important features in my model (by asking my tutors and reading articles). Coding just a skill, but the curiosity to know about your product is the key (indeed, sometimes I think it is waste of time because I feel like it is not my job! I am a coder! What I want to learn is how to build a better model and write efficient code! But, no! What is a better model? Quote from Andrew Ng, "applied machine learning is feature engineering" ).

Yeah, that is pretty much it. You can view the codes and outcome analysis in those .ipynb files (i use jupyter notebook).

sry guys, dataset too large, pls download from https://www.kaggle.com/c/home-credit-default-risk .
